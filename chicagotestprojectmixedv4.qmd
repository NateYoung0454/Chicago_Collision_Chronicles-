---
title: "Chicago Crashes Classification Modeling"
format: html
editor: visual
---

# Chicago Collision Chronicles

Question of Interest: Can we predict the severity of injuries (eg. minor, moderate, severe) in car crashes using similar conditions, (i.e. weather conditions, lighting conditions and road surface conditions?

# Table of Contents

1.  Data Overview

    1.  Data Cleaning

    2.  Mapping

2.  Regression Approaches

    1.  Simple Linear regression

    2.  Multinomial Regression

    3.  Logistic Regression

3.  Classification

    1.  Decision Trees

    2.  Linear Discriminant Analysis

    3.  Quadratic Discriminant Analysis

    4.  KNN

4.  Conclusion

5.  Appendix

```{r}
# Load packages
library(readr)
library(tidyverse)
library(MASS)
library(ISLR2)
library(leaps)
library(dplyr)
library(ggplot2)
library(caret)
library(nnet)
library(class)
library(car)
library(mapview)
library(sf)
library(data.table)
library(tree)
library(randomForest)
```

# Data Overview and Preparation

```{r}
# read data
carcrashdata <- fread("https://data.cityofchicago.org/api/views/85ca-t3if/rows.csv?accessType=DOWNLOAD")
```

```{r}
# Display variables
# variables <- colnames(carcrashdata)
# print(variables)

carcrashdata <- read_csv("Traffic_Crashes.csv", show_col_types = FALSE)
carcrashdata <- carcrashdata[complete.cases(carcrashdata$MOST_SEVERE_INJURY,
                                                  carcrashdata$WEATHER_CONDITION,
                                                  carcrashdata$LIGHTING_CONDITION,
                                                  carcrashdata$ROADWAY_SURFACE_COND), ]

# Converting key variables to factors
carcrashdata <- carcrashdata %>%
  mutate(
    WEATHER_CONDITION = factor(WEATHER_CONDITION, levels = c("CLEAR", "SNOW", "UNKNOWN", "RAIN", "CLOUDY/OVERCAST", "FOG/SMOKE/HAZE", "BLOWING SNOW", "FREEZING RAIN/DRIZZLE", "OTHER", "SEVERE CROSS WIND GATE", "SLEET/HAIL", "BLOWING SAND, SOIL, DIRT")),
    LIGHTING_CONDITION = factor(LIGHTING_CONDITION, levels = c("DAYLIGHT", "DARKNESS, LIGHTED ROAD", "DARKNESS", "UNKNOWN", "DUSK", "DAWN")),
    ROADWAY_SURFACE_COND = factor(ROADWAY_SURFACE_COND, levels = c("DRY", "UNKNOWN","SNOW OR SLUSH","WET","OTHER","ICE","SAND, MUD, DIRT")),
    DAMAGE = factor(DAMAGE, levels = c("$500 OR LESS", "$501 - $1,500", "OVER $1,500"), ordered = TRUE),
    DAMAGE = as.numeric(DAMAGE) *500)
carcrashdata$MOST_SEVERE_INJURY <- factor(carcrashdata$MOST_SEVERE_INJURY)

# Define a vector to rank the levels of MOST_SEVERE_INJURY
severity_rank <- c("NO INDICATION OF INJURY", "REPORTED, NOT EVIDENT", "NONINCAPACITATING INJURY", "INCAPACITATING INJURY", "FATAL")
# Convert levels to a factor with specified order
carcrashdata$MOST_SEVERE_INJURY <- factor(carcrashdata$MOST_SEVERE_INJURY, levels = severity_rank)

# Recode levels to binary values
carcrashdata <- carcrashdata %>%
  filter(MOST_SEVERE_INJURY != "") %>%
  mutate(MINOR_MAJOR_INJURY = case_when(
    MOST_SEVERE_INJURY %in% c("NO INDICATION OF INJURY", "REPORTED, NOT EVIDENT", "NONINCAPACITATING INJURY") ~ 0,
    MOST_SEVERE_INJURY %in% c("INCAPACITATING INJURY", "FATAL") ~ 1,
    TRUE ~ NA_integer_  # handle unexpected cases
  ))
```

```{r}
# Subset the dataset to remove NA values from MOST_SEVERE_INJURY amd others
carcrashdata <- carcrashdata[complete.cases(carcrashdata$MOST_SEVERE_INJURY,
                                                  carcrashdata$WEATHER_CONDITION,
                                                  carcrashdata$LIGHTING_CONDITION,
                                                  carcrashdata$ROADWAY_SURFACE_COND), ]
```

```{r}
# Convert MOST_SEVERE_INJURY into a factor
carcrashdata$MOST_SEVERE_INJURY <- factor(carcrashdata$MOST_SEVERE_INJURY)
```

```{r}
# Print the structure of the modified column to verify it's now an ordered factor
str(carcrashdata$MOST_SEVERE_INJURY)

carcrashdata <- carcrashdata %>%
  mutate(
    MINOR_MAJOR_INJURY = case_when(
      MOST_SEVERE_INJURY == "NO INDICATION OF INJURY" ~ 0,
      MOST_SEVERE_INJURY == "REPORTED, NOT EVIDENT" ~ 1,
      MOST_SEVERE_INJURY == "NONINCAPACITATING INJURY" ~ 2,
      MOST_SEVERE_INJURY == "INCAPACITATING INJURY" ~ 3,
      MOST_SEVERE_INJURY == "FATAL" ~ 4,
      TRUE ~ NA_integer_  # for handling any cases not specified above
    )
  )
carcrashdata <- carcrashdata %>%
  mutate(MINOR_MAJOR_INJURY = factor(MINOR_MAJOR_INJURY,levels = c(0:4),labels = c('No Injury', "REPORTED, NOT EVIDENT", "Minor Injury", "Major Injury", "FATAL"), ordered = TRUE))
carcrashdata <- carcrashdata[complete.cases(carcrashdata$MINOR_MAJOR_INJURY),]


# Remove PII and irrelevant data
carcrashdata <- carcrashdata %>%
  dplyr::select(-CRASH_RECORD_ID, -BEAT_OF_OCCURRENCE, -LANE_CNT, -TRAFFIC_CONTROL_DEVICE, -PHOTOS_TAKEN_I, -CRASH_DATE_EST_I, -DATE_POLICE_NOTIFIED, -STATEMENTS_TAKEN_I, -DEVICE_CONDITION)
# Remove NA values from key variables
carcrashdata <- carcrashdata[complete.cases(carcrashdata$MOST_SEVERE_INJURY, carcrashdata$WEATHER_CONDITION, carcrashdata$LIGHTING_CONDITION, carcrashdata$ROADWAY_SURFACE_COND, carcrashdata$INJURIES_TOTAL, carcrashdata$INJURIES_FATAL, carcrashdata$LATITUDE, carcrashdata$LONGITUDE), ]
```

```{r}
map_df(carcrashdata,~sum(is.na(.))) # Identify NA values
str(carcrashdata)
```

```{r}
ggplot(data = carcrashdata, aes(x = MOST_SEVERE_INJURY)) +
  geom_bar() +
  labs(title = 'non-fatal vs fatal injuries',x = "Type of injury",y = "Injuries")
```

There are vastly more non-lethal than lethal injuries in this data set. Predicting fatal accidents will have to have a tiny missclassification rate.

```{r}
ggplot(data = carcrashdata, aes(x = WEATHER_CONDITION)) +
  geom_bar() 
```

From our initial analysis, most car crashes occur on clear days. My theory is that worse accidents in terms of both fatalities and damage are more likely on days with bad weather.

To conduct model testing, we've separated our data into a training and testing set with 50,000 accidents in the training set and 768804 accidents in the testing set.

```{r}
# Divide into training and testing
set.seed(123)
# Sample 50,000 rows for the training set
Z <- sample(nrow(carcrashdata), 50000)
carcrash.train <- carcrashdata[Z,]
# Remaining rows for the test set
carcrash.test <- carcrashdata[-Z,]
```

## Mapping/Location

```{r}
filtered_carcrashdata <- carcrashdata %>%
  filter(!is.na(LONGITUDE), !is.na(LATITUDE), LONGITUDE != 0, LATITUDE != 0)
ggplot(filtered_carcrashdata, aes(x = LONGITUDE, y = LATITUDE, color =DAMAGE)) +
  geom_point() +
  labs(x = "Longitude", y = "Latitude", title = "Chicago accidents")
```

I created an initial map of the locations of fatal injuries using Latitude and Longitude. However since fatal accidents are so far outnumbered by non-fatal accidents we don't learn much from this plot. To indicate locations where crashes are more severe, I plotted the severity of injury. We can now see areas where costs are low and high. However this is a challenge to pinpoint using just ggplot.

```{r}
ggplot(filtered_carcrashdata, aes(x = LONGITUDE, y = LATITUDE, color = MINOR_MAJOR_INJURY)) +
  geom_point() +
  labs(x = "Longitude", y = "Latitude", title = "Chicago accidents cost")
```

Using the mapview package, we can isolate fatal crashes on an interactive map of Chicago.

```{r}
lethal_filtered_carcrashdata <- carcrashdata %>%
  filter(!is.na(LONGITUDE), !is.na(LATITUDE), LONGITUDE != 0, LATITUDE != 0, INJURIES_FATAL == 1)
mapview(lethal_filtered_carcrashdata, xcol = "LONGITUDE", ycol = "LATITUDE", crs = 4269, grid = FALSE) 
```

But can we create a model to predict the location of fatal accidents? We will examine location data alongside other variables over the course of this project.

# Regression

## Simple Linear regression (add plots)

We began with simple linear regression with relevant variables. When running linear models proposing all variables I get the error message: "vector memory exhausted (limit reached?)". This is due to the sheer number of variables included in the initial selection process. I also have this error when running the same test on just training data. For this reason, I've conducted variable selection manually.

```{r}
levels(carcrash.train$ROAD_DEFECT)

```

```{r}
Var_select_Model <- lm(MINOR_MAJOR_INJURY ~ POSTED_SPEED_LIMIT + ROADWAY_SURFACE_COND + ROAD_DEFECT + CRASH_TYPE + INTERSECTION_RELATED_I + HIT_AND_RUN_I + DAMAGE + WORK_ZONE_I + NUM_UNITS + MOST_SEVERE_INJURY + INJURIES_TOTAL + INJURIES_FATAL + INJURIES_INCAPACITATING + INJURIES_NON_INCAPACITATING + CRASH_DAY_OF_WEEK + CRASH_HOUR + CRASH_MONTH + LATITUDE + LONGITUDE, data = carcrash.train)
summary(Var_select_Model)
```

From the initial variable selection model variables related to road conditions had more impact than defects in the road. Hit and runs are also highly correlated with injury. Additionally date, time and geolocation had little to do with predicting injury. Next lets drop some of the non-useful variables.

```{r}
# Injury prediction models
Roadway_Model <- lm(MINOR_MAJOR_INJURY ~ INTERSECTION_RELATED_I+ POSTED_SPEED_LIMIT+ ROADWAY_SURFACE_COND, data = carcrashdata)
summary(Roadway_Model)
```

While the model focusing on road conditions is statistically significant, it doesn't account for much variance in injuries. Let's look at some models examining different types of variables.

```{r}
# Time Model
time_model_fatal <- lm(MINOR_MAJOR_INJURY ~ CRASH_DAY_OF_WEEK + CRASH_HOUR + CRASH_MONTH, data = carcrashdata)
summary(time_model_fatal)
```

I built a model related to just the variables related to date and time. As expected, there is little relationship between time and severity of injury.

```{r}
# Damage Cost Model
damage_model1 <- lm(MINOR_MAJOR_INJURY ~ INJURIES_TOTAL + DAMAGE + MOST_SEVERE_INJURY, data = carcrashdata)
summary(damage_model1)
```

The initial model I built focusing on elements related to damage and injuries has an R-square of 1. This is too good a fit with many degrees of freedom. Let's reduce it further.

```{r}
# Damage Cost Model
damage_model2 <- lm(MINOR_MAJOR_INJURY ~ INJURIES_TOTAL + DAMAGE, data = carcrashdata)
summary(damage_model2)
```

The second model focusing on damage is no longer over-fitting the data and has a relativly respectable R square. Damage and number of injures. However, we can do better by applying more advanced regression and classification techniques.

## Multinomial Regression

```{r}
# Fit a multinomial logistic regression model
carcrash.model <- multinom(MOST_SEVERE_INJURY ~ WEATHER_CONDITION + LIGHTING_CONDITION + ROADWAY_SURFACE_COND, data = carcrash.train)

# Print model summary
summary(carcrash.model)
```

```{r}
# Predict probabilities for each level of injury severity in the test data
carcrash.predict <- predict(carcrash.model, newdata = carcrash.test, type = "probs")

# Calculate the average predicted probabilities for each level of injury severity
avg_probabilities <- colMeans(carcrash.predict)
avg_probabilities

```

```         
```

```{r}
# Fit logistic regression model
logit_model <- glm(MINOR_MAJOR_INJURY ~ WEATHER_CONDITION + LIGHTING_CONDITION + ROADWAY_SURFACE_COND, 
                   data = carcrash.train, 
                   family = binomial)

# Summary of logistic regression model
summary(logit_model)
```

## Logistic Regression model for Fatal Accidents

Next, to predict the likelihood of fatal accidents, we utilizeed logistic regression. We started with transforming the dataset by creating the new variable which shows if the injury from the accident resulted in the fatal. After training the logistic regression model on the training data, it predicts the probability of fatal accidents on the test data. Finally, I evaluated the model's performance by calculating the misclassification rate.

```{r}
carcrashdata <- carcrashdata %>%
  mutate(FATAl_ACCIDENT = ifelse(INJURIES_FATAL > 0, 1, 0))
FATAL_Logistic <- glm(FATAl_ACCIDENT ~ CRASH_HOUR  + LIGHTING_CONDITION + POSTED_SPEED_LIMIT + FIRST_CRASH_TYPE, 
             family = binomial, data = carcrashdata)
summary(FATAL_Logistic)

# Converting FIRST_CRASH_TYPE variable to factor
unique(carcrashdata$FIRST_CRASH_TYPE)
carcrashdata <- carcrashdata %>%
  mutate(
    FIRST_CRASH_TYPE = factor(FIRST_CRASH_TYPE, levels = c("REAR END", "PARKED MOTOR VEHICLE", "PEDALCYCLIST", "PEDESTRIAN", "FIXED OBJECT", 
                             "TURNING", "ANGLE", "SIDESWIPE SAME DIRECTION", "SIDESWIPE OPPOSITE DIRECTION", 
                             "REAR TO SIDE", "HEAD ON", "REAR TO FRONT", "OTHER OBJECT", "ANIMAL", 
                             "OTHER NONCOLLISION", "OVERTURNED", "REAR TO REAR", "TRAIN")
))
unique(carcrashdata$TRAFFICWAY_TYPE)

carcrashdata <- carcrashdata %>%
  mutate(
    TRAFFICWAY_TYPE = factor(TRAFFICWAY_TYPE, levels = c("OTHER", "DIVIDED - W/MEDIAN (NOT RAISED)", "NOT DIVIDED", "ONE-WAY",
                                   "FOUR WAY", "PARKING LOT", "DIVIDED - W/MEDIAN BARRIER", "T-INTERSECTION",
                                   "UNKNOWN", "RAMP", "ALLEY", "DRIVEWAY", "UNKNOWN INTERSECTION TYPE",
                                   "TRAFFIC ROUTE", "FIVE POINT, OR MORE", "NOT REPORTED", "CENTER TURN LANE",
                                   "L-INTERSECTION", "Y-INTERSECTION", "ROUNDABOUT")
    ))
# Filter out observations with missing values 
carcrashdata_complete <- carcrashdata[!is.na(carcrashdata$FATAl_ACCIDENT), ]

set.seed(123) 
train_index <- sample(nrow(carcrashdata_complete), 0.8 * nrow(carcrashdata_complete))  # 80% train, 20% test
train_data <- carcrashdata_complete[train_index, ]
test_data <- carcrashdata_complete[-train_index, ]

model <- glm(FATAl_ACCIDENT ~ CRASH_HOUR + LIGHTING_CONDITION + POSTED_SPEED_LIMIT + FIRST_CRASH_TYPE, 
             family = binomial, data = train_data)

predicted <- predict(model, newdata = test_data, type = "response")

predicted_class <- ifelse(predicted > 0.5, 1, 0)  
actual_class <- test_data$FATAl_ACCIDENT 

logisticmisclassification_rate <- mean(predicted_class != actual_class)
print(paste("Misclassification Rate:", logisticmisclassification_rate))
```

Logistic Regression produced a missclassication rate of 0.00111007988935597.

# Classification

## Decision Tree for predicting most severe injury

```{r}
set.seed(123)
tr <- tree(as.factor(MOST_SEVERE_INJURY) ~ ., data = carcrash.train)
summary(tr)
tr
plot(tr)
text(tr)
cv.tree(tr, FUN = prune.misclass)
```

The optimal classification tree size is 5 or lower.

```{r}
set.seed(123)
cv <- cv.tree(tr, FUN = prune.misclass)
plot(cv)
decisiontree5missclass <- 42/50000
```

```{r}
trp <- prune.misclass(tr, best = 4)
summary(trp)
decisiontree4missclass <- 203/50000
```

During cross-validation, the optimal tree size is 5 although 4 appears to be not far behind. When we reduce the tree size to 4, the missclassification rate increases from 0.001191 to 0.003875. While this is still low. I recommend the tree of size 5.

```{r}
trpp <- rpart::rpart(MOST_SEVERE_INJURY ~ INJURIES_TOTAL + INJURIES_NON_INCAPACITATING + MINOR_MAJOR_INJURY, data = carcrash.train)
rpart.plot::rpart.plot(trpp)
```

```{r}
ggplot(carcrash.train, aes(y = MOST_SEVERE_INJURY, x = INJURIES_NON_INCAPACITATING, color = MINOR_MAJOR_INJURY)) +
  geom_point() 
```

The best predictor of fatal injuries is the presence or lack thereof of more minor injuries.

### Using Decision Trees to predict hit and runs

While not necesarily fatal, hit and runs are more likely to be dangerous than other accidents. I've built the decision tree below to predict what variables are most associated with hit and runs.

```{r}
treehitandrun <- tree(as.factor(HIT_AND_RUN_I) ~ ., data = carcrash.train)
summary(treehitandrun)
treehitandrun
plot(treehitandrun)
text(treehitandrun)
cv.tree(treehitandrun, FUN = prune.misclass)
```

Running a decision tree to predict if a car crash is a hit and run can be determined by the variables related to number of injuries and number of responding police units. It makes sense that more police will be assigned to a hit and run crash than other crashes.

## LDA

```{r}
# Fit LDA model
lda_model <- lda(MINOR_MAJOR_INJURY ~ WEATHER_CONDITION + LIGHTING_CONDITION + ROADWAY_SURFACE_COND, data = carcrashdata)

# Summary of LDA model
summary(lda_model)

# Predict using the LDA model
predictions <- predict(lda_model, newdata = carcrash.test)

####### head(predictions) ## very long

# If you want the posterior probabilities of each class, you can also extract them
posterior_probs <- predictions$posterior
# head(posterior_probs) ####### Recommend against printing all posterior probabilities (long) unless we graph them

# Calculate average posterior probabilities for each class
avg_posterior_class_0 <- mean(posterior_probs[, "0"])
avg_posterior_class_1 <- mean(posterior_probs[, "1"])

# Output the average posterior probabilities
avg_posterior_class_0
avg_posterior_class_1

# Alternatively, if you just want the predicted class for each observation, you can use the type = "response" argument
predicted_classes <- predict(lda_model, newdata = carcrash.test, type = "response")
```

Creating an LDA model with many more variables results in collinearity and an ineffective model.

```{r}
lda_model1 <- lda(MINOR_MAJOR_INJURY ~ ROADWAY_SURFACE_COND + ROAD_DEFECT + CRASH_TYPE + INTERSECTION_RELATED_I + HIT_AND_RUN_I + DAMAGE + WORK_ZONE_I + NUM_UNITS + MOST_SEVERE_INJURY + INJURIES_TOTAL + INJURIES_FATAL + INJURIES_INCAPACITATING + INJURIES_NON_INCAPACITATING + CRASH_DAY_OF_WEEK + CRASH_HOUR + CRASH_MONTH + LATITUDE + LONGITUDE, data = carcrash.train) 

Predicted.fatal_lda <- predict(lda_model1, data.frame(carcrash.test))$class

table(carcrash.test$MINOR_MAJOR_INJURY, Predicted.fatal_lda)
round(mean(carcrash.test$MINOR_MAJOR_INJURY == Predicted.fatal_lda), 3) # Classification Rate
round(mean(Predicted.fatal_lda == "Up"), 3)
round(mean(carcrash.test$MINOR_MAJOR_INJURY == "Up"), 3)
```

Lets reduce the model using what we learned from variable selection earlier.

```{r}
lda_model2 <- lda(MINOR_MAJOR_INJURY ~ ROADWAY_SURFACE_COND + INTERSECTION_RELATED_I + HIT_AND_RUN_I + DAMAGE + NUM_UNITS + MOST_SEVERE_INJURY + INJURIES_TOTAL + INJURIES_NON_INCAPACITATING, data = carcrash.train) 

Predicted.fatal_lda <- predict(lda_model2, data.frame(carcrash.test))$class

table(carcrash.test$MINOR_MAJOR_INJURY, Predicted.fatal_lda)
round(mean(carcrash.test$MINOR_MAJOR_INJURY == Predicted.fatal_lda), 9) # Classification Rate
```

This is the best model yet with a correct classification rate of 99.87526%. This incorporates several types of variables to reduce risk of multicollinearity. To test this, we will examine the Variance Inflation Factor (VIF) of this model.

```{r}
# Calculate VIF
vif(lm(MINOR_MAJOR_INJURY ~ ROADWAY_SURFACE_COND + INTERSECTION_RELATED_I + HIT_AND_RUN_I + DAMAGE + NUM_UNITS + MOST_SEVERE_INJURY + INJURIES_TOTAL + INJURIES_NON_INCAPACITATING, data = carcrash.train))
```

We have respectable VIF for everything but most severe injury. We should remove this from the model.

```{r}
lda_model3 <- lda(MINOR_MAJOR_INJURY ~ ROADWAY_SURFACE_COND + INTERSECTION_RELATED_I + HIT_AND_RUN_I + DAMAGE + NUM_UNITS + INJURIES_TOTAL + INJURIES_NON_INCAPACITATING, data = carcrash.train) 

Predicted.fatal_lda <- predict(lda_model3, data.frame(carcrash.test))$class

table(carcrash.test$MINOR_MAJOR_INJURY, Predicted.fatal_lda)
LDAclass<-round(mean(carcrash.test$MINOR_MAJOR_INJURY == Predicted.fatal_lda), 9) 
LDAclass
# New VIF
vif(lm(MINOR_MAJOR_INJURY ~ ROADWAY_SURFACE_COND + INTERSECTION_RELATED_I + HIT_AND_RUN_I + DAMAGE + NUM_UNITS + INJURIES_TOTAL + INJURIES_NON_INCAPACITATING, data = carcrash.train))
```

Our new correct classification rate is 97.33% without any issues with high VIF. Lets see if this model differs when performing Quadratic Discriminant Analysis (QDA).

For the purposes of outlining LDA in graph form, I've prepared a reduced plot based on predicting our injury response based on the number of police units responding and the total number of injuries.

```{r}
# LDA graph
lda_model_simplified <- lda(MINOR_MAJOR_INJURY ~ NUM_UNITS + INJURIES_TOTAL, data = carcrash.train)

grid <- expand.grid(
  NUM_UNITS = seq(min(carcrash.train$NUM_UNITS), max(carcrash.train$NUM_UNITS), length.out = 100),
  INJURIES_TOTAL = seq(min(carcrash.train$INJURIES_TOTAL), max(carcrash.train$INJURIES_TOTAL), length.out = 100)
)
grid$MINOR_MAJOR_INJURY <- predict(lda_model_simplified, newdata = grid)$class
carcrash.train$predicted_class <- predict(lda_model_simplified, carcrash.train)$class

# Plott decision boundaries
ggplot() +
  geom_tile(data = grid, aes(x = NUM_UNITS, y = INJURIES_TOTAL, fill = MINOR_MAJOR_INJURY), alpha = 0.5) +
  geom_point(data = carcrash.train, aes(x = NUM_UNITS, y = INJURIES_TOTAL, color = predicted_class)) +
  scale_fill_manual(values = c("red", "blue")) +
  scale_color_manual(values = c("red", "blue")) +
  labs(title = "LDA Decision Boundaries", x = "NUM_UNITS", y = "INJURIES_TOTAL")
```

There is a very clear decision boundary for the LDA graph, values for more than 3 injuries are likely to have fatalities.

## QDA

Let's run a QDA model using the same variables as our best LDA model.

```{r}
qda_model1 <- qda(MINOR_MAJOR_INJURY ~ ROADWAY_SURFACE_COND + INTERSECTION_RELATED_I + HIT_AND_RUN_I + DAMAGE + NUM_UNITS + INJURIES_TOTAL + INJURIES_NON_INCAPACITATING, data = carcrash.train) 

Predicted.fatal_qda <- predict(qda_model1, data.frame(carcrash.test))$class

table(carcrash.test$MINOR_MAJOR_INJURY, Predicted.fatal_qda)
QDAclass<- round(mean(carcrash.test$MINOR_MAJOR_INJURY == Predicted.fatal_qda), 9)
QDAclass
```

The QDA model performs worse than the LDA model with the same variables. It has a correct classification rate of 0.9362815.

While we can't graph all the variables from the QDA model, we can graph 2. I've decided to graph the number of responding police units and number of total injuries as these have the greatest range.

```{r}
# QDA graph
qda_model_simplified <- qda(MINOR_MAJOR_INJURY ~ NUM_UNITS + INJURIES_TOTAL, data = carcrash.train)

grid <- expand.grid(
  NUM_UNITS = seq(min(carcrash.train$NUM_UNITS), max(carcrash.train$NUM_UNITS), length.out = 100),
  INJURIES_TOTAL = seq(min(carcrash.train$INJURIES_TOTAL), max(carcrash.train$INJURIES_TOTAL), length.out = 100)
)
grid$MINOR_MAJOR_INJURY <- predict(qda_model_simplified, newdata = grid)$class
carcrash.train$predicted_class <- predict(qda_model_simplified, carcrash.train)$class

# Plott decision boundaries
ggplot() +
  geom_tile(data = grid, aes(x = NUM_UNITS, y = INJURIES_TOTAL, fill = MINOR_MAJOR_INJURY), alpha = 0.5) +
  geom_point(data = carcrash.train, aes(x = NUM_UNITS, y = INJURIES_TOTAL, color = predicted_class)) +
  scale_fill_manual(values = c("red", "blue")) +
  scale_color_manual(values = c("red", "blue")) +
  labs(title = "QDA Decision Boundaries", x = "NUM_UNITS", y = "INJURIES_TOTAL")

```

The fewer police units and number of injuries, the less likely there is a fatal crash.

## KNN

I tried for many hours to perform KNN on this dataset but could not get beyond this error message: "Error in knn(train = train_set, test = test_set, cl = train_labels, k = 7) : 'train' and 'class' have different lengths". Even when they were the same length and after consulting stackexchange, this could not be fixed.

Don't make my mistake, save yourself the time and headache. Don't try it with this dataset.

# Write up/Summary

-   3 Pts: Identify the data source, describe the original data, and any challenges or choices in cleaning the data for analysis.

    -   The first big challenge with data cleaning was converting key variables from characters to factors. The initial analyses for code returned many errors until the variables were recoded. KNN was impossible with this dataset due to an error that could not be circumvented.

-   3 Pts: Identify the stakeholders in the analysis and its outcomes. Assess any ethical implications of the data (collection methods, sources, structure) or the choices made in the analysis (grouping, selection, etc.) or any other responsible data science concerns for implementation.

    -   During the initial variable selection, I thought it was appropriate to form initial models around various themes in the data such as location, damage and environment. None of these could account for a comprehensive model on their own. The best model included elements of each of these categories.

-   2 Pts: Summarize Findings

    -   The best approach proved to be LDA with using variables including road conditions, intersection, cost of accident, and some injury information. The ideal model shouldn't overfit the training data. We learn from the research process at each stage including, variable selection, model building and method.

```{r}
tibble("Model" = c("Unpruned Decision Tree", "Pruned Decision Tree", "LDA", 
         "QDA", 'Logistic Regression'),
       "Classification Rate" = c(decisiontree5missclass, decisiontree4missclass, LDAclass, QDAclass, logisticmisclassification_rate)) |> 
  arrange(desc(`Classification Rate`)) |> 
  gt::gt() |> 
  gt::opt_row_striping(row_striping = TRUE) |> 
  gt::tab_options(row.striping.background_color = "lightgreen",
                  column_labels.background.color = "lightblue")
```

-   1 Pts: Offer recommendations for implementation or additional work

    -   I would love to perform more location based analysis compared location to road and weather conditions using mapview.

# Appendix

| Group Member  | Responsabilities                                                                  |
|-----------------------|------------------------------------------------|
| Kevin Norris  | Introduction, LDA, Logistic Regression, Majority of writing, editing, formatting. |
| Nathan Young  | Simple Linear Regression, Decision tree, mapping, LDA, QDA, Github, some writing. |
| Yota          | Multinomial Regression                                                            |
| Murtaza Jawid | Editing                                                                           |
